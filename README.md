# Gracenote
An visual-to-auditory prosthetic developed in the Human 2.0 Class at the MIT Media Lab

This is an unorganized repository of everything our team used to create a protypte of an audio-visual prosthetic using three HC-SR04 ultrasonic depth sensors.

## Includes

* The Arduino file to use map distance from the three ultrasonic sensors to various MIDI sounds using the library that Adafruit ships
* The OpenFrameworks application we used to use stereo sounds to see if people can orient themselves between two walls
* The iPython file we used to process the data and produce LaTeX friendly figure

Team was: Chrisoula Kapeloni, Tiffany Kuo, Greg Lubin, Daniel Goodwin

Post an issue if you'd like any help using this code. 
